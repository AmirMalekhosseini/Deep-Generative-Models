{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e75tOOnAJRzV"
   },
   "source": [
    "# **HW2-P2: StyleGAN2-Ada**\n",
    "\n",
    "In this assignment, you will work with a state-of-the-art generative model called **StyleGAN2-Ada** ([Karras et al., 2020a](https://arxiv.org/abs/2006.06676))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuCBTV6GvQut"
   },
   "source": [
    "\n",
    "## **Background**\n",
    "\n",
    "### **StyleGAN**\n",
    "\n",
    "To understand StyleGAN2-Ada, we first revisit the **StyleGAN** architecture ([Karras et al., 2019](https://arxiv.org/abs/1812.04948)). Unlike a standard GAN, StyleGAN’s generator is designed to provide **control over different levels of visual detail**, such as:\n",
    "\n",
    "<br/>\n",
    "\n",
    "| Detail Level | Examples |\n",
    "|-------------|----------|\n",
    "| Fine        | Hair strands, freckles |\n",
    "| Mid         | Eye openness, hairstyle |\n",
    "| Coarse      | Face shape, head pose, glasses |\n",
    "\n",
    "<br/>\n",
    "\n",
    "This control comes from the generator architecture shown in Figure 1 of the paper.\n",
    "\n",
    "StyleGAN first maps a latent code $z \\in \\mathbb{R}^{512}$  into an **intermediate latent** $w \\in \\mathbb{R}^{512}$.  The synthesis network then gradually converts \\( w \\) into an image by progressively increasing resolution (from $4 \\times 4$ to $1024 \\times 1024$), allowing different network layers to control fine, medium, and coarse features.\n",
    "\n",
    "**Additional techniques to improve training include:**\n",
    "\n",
    "- WGAN-GP objective (Arjovsky et al., 2017)\n",
    "- Adaptive Instance Normalization (AdaIN)\n",
    "- Truncation trick in latent space (Brock et al., 2018)\n",
    "\n",
    "### **StyleGAN2**\n",
    "\n",
    "StyleGAN sometimes produces undesirable distortions such as **“water droplet” / “blob-like” visual artifacts**, caused by AdaIN and progressive upsampling. StyleGAN2 resolves this by:\n",
    "\n",
    "- Replacing AdaIN with **demodulation operation** (removing artifacts)\n",
    "- Replacing progressive upsampling with **skip connections** (improving stability)\n",
    "\n",
    "### **StyleGAN2-Ada**\n",
    "\n",
    "Training StyleGAN2 from scratch requires millions of images and weeks of GPU time.  However, training on small datasets normally causes discriminator overfitting. StyleGAN2-Ada introduces **Adaptive Discriminator Augmentation (ADA)**, which automatically adjusts augmentation strength during training. This enables **high-quality training using only a few thousand images**, while maintaining StyleGAN2’s output quality.\n",
    "\n",
    "In this assignment, we will use a **pre-trained StyleGAN2-Ada generator**.\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FoPoJk5CvQuu"
   },
   "source": [
    "## **Experiments**\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3FsP6IevQuu"
   },
   "source": [
    "### 1.  **Sampling and Identifying Fake Images**\n",
    "\n",
    "Your goal is to generate a small row of 3–5 images using a pre-trained model.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1.  **Unlock Generator:** Choose and unlock one of the pre-trained generators.\n",
    "2.  **Complete Functions:** Implement the `generate_latent_code` and `generate_images` functions.\n",
    "3.  **Follow Documentation:** To complete these functions, follow the instructions in this notebook and refer to the [Official PyTorch implementation](https://github.com/NVlabs/stylegan2-ada-pytorch.git)\n",
    "4.  **(optional) Review Images:** Once your images are generated, you can use [Which Face Is Real?](https://www.whichfaceisreal.com/learn.html) as a guideline to help you spot any imperfections.\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzP1qhAJvQuu"
   },
   "source": [
    "### 2. **Latent Space Interpolation**\n",
    "\n",
    "Your goal is to complete the `interpolate_images` function. This will generate a sequence of images that smoothly transitions between two random faces.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1.  **Get Latent Vectors:** Use your `generate_latent_code` function (from Part 1) to create two separate latent vectors, $z_1$ and $z_2$.\n",
    "2.  **Implement Interpolation:** Linearly interpolate between $z_1$ and $z_2$ using the formula below. You'll need to create several steps for the value $r$ (e.g., `0, 0.1, 0.2, ... 1.0`) to create a smooth transition.\n",
    "\n",
    "    $$z = r z_{1} + (1 - r) z_{2}, \\quad r \\in [0, 1]$$\n",
    "\n",
    "3.  **Generate Images:** Feed each resulting interpolated latent code ($z$) into the StyleGAN2-Ada generator. This will produce the final sequence of images.\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-kkAdjhJRzV"
   },
   "source": [
    "### 3. **Style Mixing and Fine Detail Control**\n",
    "\n",
    "In this final part, your goal is to reproduce the famous style mixing example from the original StyleGAN paper (Figure 3). This involves generating images using specific latent codes at different levels of the synthesis network.\n",
    "\n",
    "<br/>\n",
    "\n",
    "#### Step 1: **Generate W-space latents from Z-space seeds**\n",
    "\n",
    "Your first task is to convert random Z-space vectors (from seeds) into W-space latents using the generator’s **mapping network**. These W-space latents (`w`) are used in later steps for style mixing.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Complete the `latent_to_w` function to map latent codes from Z-space to W-space.  \n",
    "2. Apply the **truncation trick**:  \n",
    "   - Compute the average W latent (`w_avg`) from the mapping network.  \n",
    "   - Truncate each latent using:  \n",
    "\n",
    "     $$\n",
    "     w' = w_{avg} + (w - w_{avg}) \\times \\psi\n",
    "     $$\n",
    "\n",
    "     where $\\psi$ is your truncation constant (recommended 0.7).  \n",
    "\n",
    "3. The resulting W-space latents will be used as input for the synthesis network (`G_ema.synthesis`) to generate images.\n",
    "\n",
    "<br/>\n",
    "\n",
    "#### Step 2: **Generate images from subsets of the generator (coarse, middle, fine styles)**\n",
    "\n",
    "Next, use your W-space latents to generate images from different **subsets of layers** in the generator. This allows you to see how different layers influence specific aspects of the image.\n",
    "\n",
    "- **Coarse layers:** control high-level structure (pose, face shape).  \n",
    "- **Middle layers:** control medium-level features (facial features, hair style).  \n",
    "- **Fine layers:** control textures and colors.  \n",
    "\n",
    "**Task:** Implement the `w_to_image` function\n",
    "\n",
    "<br/>\n",
    "\n",
    "#### Step 3: **Generate and Analyze the Style-Mixed Grid**\n",
    "\n",
    "Now you will use your new function to create the mixing grid and analyze the results.\n",
    "\n",
    "**Part 3a: Generate the Grid**\n",
    "\n",
    "1.  Use the code from `style_mixing` into the indicated location in the final cell.\n",
    "2.  Initialize the `col_seeds`, `row_seeds`, and `col_styles` variables as instructed.\n",
    "3.  Run the final cell to generate the grid of images.\n",
    "\n",
    "**Part 3b: Your Analysis (Deliverable)**\n",
    "\n",
    "After generating the grid, experiment by changing the `col_styles` variable. Then, write your analysis.\n",
    "\n",
    "1.  In a few sentences, **explain what the `col_styles` variable does.** Describe what the different numbers in the list correspond to (Hint: think about coarse vs. fine details).\n",
    "2.  Create a **simple experiment** to back up your explanation.\n",
    "3.  Include **one or two sets of images** (as screenshots) that clearly illustrate the effect of changing `col_styles` and support your argument.\n",
    "\n",
    "<br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yF4cjks-LIjj"
   },
   "source": [
    "**Note: To run this notebook efficiently in Google Colab, make sure to use T4 GPU.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-W5eReUVPnS"
   },
   "outputs": [],
   "source": [
    "# Install StyleGAN2-ADA PyTorch\n",
    "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n",
    "%cd stylegan2-ada-pytorch\n",
    "\n",
    "# Install dependencies\n",
    "!pip install torch torchvision ninja imageio-ffmpeg tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "iVNM_ERtVxA1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import StyleGAN2-ADA PyTorch modules\n",
    "from torch_utils import misc\n",
    "from training import networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhU-cjsC6CdN"
   },
   "source": [
    "Next, we will load a pre-trained StyleGan2-ada network.\n",
    "\n",
    "Each of the following pre-trained network is specialized to generate one type of image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ENrDl7ZTddyT"
   },
   "outputs": [],
   "source": [
    "# The pre-trained networks are stored as standard pickle files\n",
    "# Use one of the following URL to begin\n",
    "\n",
    "# Human faces: https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\n",
    "# CIFAR-10 tiny images: https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/cifar10.pkl\n",
    "# European portrait paintings: https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl\n",
    "# Cats: https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/afhqcat.pkl\n",
    "# Dogs: https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/afhqdog.pkl\n",
    "\n",
    "!wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuScDftRtbdy"
   },
   "outputs": [],
   "source": [
    "pkl_file = '/content/stylegan2-ada-pytorch/ffhq.pkl'  # or any other pretrained model\n",
    "with open(pkl_file, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# In PyTorch StyleGAN2-ADA, usually you get:\n",
    "# 'G_ema' : EMA (average) generator\n",
    "# 'G'     : instantaneous generator (for training)\n",
    "# 'D'     : discriminator\n",
    "G_ema = data['G_ema'].cuda().eval()   # long-term average generator\n",
    "G     = data['G'].cuda().eval()       # instantaneous generator\n",
    "D     = data['D'].cuda().eval()       # discriminator\n",
    "\n",
    "print(G_ema, G, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "y5SpkSs8tjjy"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore warnings and failiers. These are optional performance optimizations.\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch_utils.ops.upfirdn2d\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torch_utils.ops.bias_act\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mk3EKnIyckPE"
   },
   "source": [
    "### 1.  **Sampling and Identifying Fake Images**\n",
    "\n",
    "Your goal is to generate a small row of 3–5 images using a pre-trained model.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1.  **Generator:** Choose one of the pre-trained generators.\n",
    "2.  **Complete Functions:** Implement the `generate_latent_code` and `generate_images` functions.\n",
    "3.  **Follow Documentation:** To complete these functions, follow the instructions in this notebook and refer to the [Official PyTorch implementation](https://github.com/NVlabs/stylegan2-ada-pytorch.git)\n",
    "4.  **Review Images:** Once your images are generated, you can use [Which Face Is Real?](https://www.whichfaceisreal.com/learn.html) as a guideline to help you spot any imperfections.\n",
    "\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "N7eAfVJDiB0y"
   },
   "outputs": [],
   "source": [
    "# Sample a batch of latent codes {z_1, ...., z_B}, B is your batch size.\n",
    "def generate_latent_code(SEED, BATCH, LATENT_DIMENSION=512):\n",
    "    \"\"\"\n",
    "    Generate a batch of random latent codes.\n",
    "\n",
    "    Args:\n",
    "        SEED (int): random seed\n",
    "        BATCH (int): number of latent codes to generate\n",
    "        LATENT_DIMENSION (int): dimensionality of latent vector (default 512)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: latent codes of shape [BATCH, LATENT_DIMENSION]\n",
    "    \"\"\"\n",
    "\n",
    "    # Set the random seed for reproducibility\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    # Generate random latent codes from a normal distribution\n",
    "    latent_codes = np.random.randn(BATCH, LATENT_DIMENSION).astype(np.float32)\n",
    "\n",
    "    return torch.from_numpy(latent_codes).cuda()  # move to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "N8giN3BfibqG"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_images(SEED, BATCH=3, TRUNCATION=0.7):\n",
    "    \"\"\"\n",
    "    Generate a batch of images from latent codes using the PyTorch StyleGAN2-ADA generator.\n",
    "\n",
    "    Args:\n",
    "        SEED (int): random seed for latent codes\n",
    "        BATCH (int): number of images to generate\n",
    "        TRUNCATION (float): truncation psi (recommended 0.7)\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image: concatenated row of generated images\n",
    "    \"\"\"\n",
    "    latent_codes = generate_latent_code(\n",
    "        SEED, BATCH)  # Use the function from previous cell\n",
    "\n",
    "    # Generate images using the generator\n",
    "    # First get the W space latents using the mapping network with truncation\n",
    "    w_latents = G_ema.mapping(latent_codes, None, truncation_psi=TRUNCATION)\n",
    "    # Then generate images using the synthesis network\n",
    "    images = G_ema.synthesis(w_latents, noise_mode='const')\n",
    "\n",
    "    # Convert from tensor to numpy and adjust range from [-1, 1] to [0, 255]\n",
    "    images = (images.permute(0, 2, 3, 1) * 127.5 +\n",
    "              128).clamp(0, 255).to(torch.uint8).cpu().numpy()\n",
    "\n",
    "    # Concatenate images horizontally\n",
    "    concatenated = np.concatenate(images, axis=1)\n",
    "    return Image.fromarray(concatenated)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bdaxzuAFcTM8"
   },
   "outputs": [],
   "source": [
    "# Generate and display your images\n",
    "generate_images(SEED=14, BATCH=5, TRUNCATION=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is in Part1_Output.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9Jre5OXbUHD"
   },
   "source": [
    "### 2. **Latent Space Interpolation**\n",
    "\n",
    "Your goal is to complete the `interpolate_images` function. This will generate a sequence of images that smoothly transitions between two random faces.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1.  **Get Latent Vectors:** Use your `generate_latent_code` function (from Part 1) to create two separate latent vectors, $z_1$ and $z_2$.\n",
    "2.  **Implement Interpolation:** Linearly interpolate between $z_1$ and $z_2$ using the formula below. You'll need to create several steps for the value $r$ (e.g., `0, 0.1, 0.2, ... 1.0`) to create a smooth transition.\n",
    "\n",
    "    $$z = r z_{1} + (1 - r) z_{2}, \\quad r \\in [0, 1]$$\n",
    "\n",
    "3.  **Generate Images:** Feed each resulting interpolated latent code ($z$) into the StyleGAN2-Ada generator. This will produce the final sequence of images.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**Submission Notes:**\n",
    "\n",
    "* Include a **small row of your interpolation images** in your final notebook for submission.\n",
    "* If submission file size becomes too large, you may also screenshot the results, erase the cell output, and simply add the screenshot to the final ZIP for uploading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "fUwCVd9mlDOr"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def interpolate_images(SEED1, SEED2, INTERPOLATION=6, TRUNCATION=0.7):\n",
    "    \"\"\"\n",
    "    Linearly interpolate between two latent codes and generate images.\n",
    "\n",
    "    Args:\n",
    "        SEED1 (int): random seed for first latent code\n",
    "        SEED2 (int): random seed for second latent code\n",
    "        INTERPOLATION (int): number of interpolated steps (recommended 6-10)\n",
    "        TRUNCATION (float): truncation psi (recommended 0.7)\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image: concatenated row of interpolated images\n",
    "    \"\"\"\n",
    "    latent_code_1 = generate_latent_code(\n",
    "        SEED1, 1)  # Generate first latent code\n",
    "    latent_code_2 = generate_latent_code(\n",
    "        SEED2, 1)  # Generate second latent code\n",
    "\n",
    "    # Create interpolation steps\n",
    "    r_values = np.linspace(0, 1, INTERPOLATION)\n",
    "\n",
    "    # List to store interpolated latent codes\n",
    "    interpolated_latents = []\n",
    "\n",
    "    for r in r_values:\n",
    "        interpolated_z = r * latent_code_1 + (1 - r) * latent_code_2\n",
    "        interpolated_latents.append(interpolated_z)\n",
    "\n",
    "    # Concatenate all interpolated latent codes\n",
    "    all_interpolated = torch.cat(interpolated_latents, dim=0)\n",
    "\n",
    "    # Generate images using the generator\n",
    "    w_latents = G_ema.mapping(all_interpolated, None,\n",
    "                              truncation_psi=TRUNCATION)\n",
    "    # Then generate images using the synthesis network\n",
    "    generated_images = G_ema.synthesis(w_latents, noise_mode='const')\n",
    "\n",
    "    images = (generated_images.permute(0, 2, 3, 1) * 127.5 +\n",
    "              128).clamp(0, 255).to(torch.uint8).cpu().numpy()\n",
    "\n",
    "    concatenated = np.concatenate(images, axis=1)\n",
    "    return Image.fromarray(concatenated)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fV4Df5J8lZLy"
   },
   "outputs": [],
   "source": [
    "# Create an interpolation of generated images\n",
    "interpolate_images(SEED1=14, SEED2=17, INTERPOLATION=6, TRUNCATION=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Output is in Part2_Output.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5HUpzB0T5LN"
   },
   "source": [
    "**Note:** After you have generated interpolated images, an interesting task would be to see how you can create a GIF. Feel free to explore a little bit more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CV2tsaAhaTzg"
   },
   "source": [
    "## Part 3: **Style Mixing and Fine Control**\n",
    "\n",
    "In this final part, your goal is to reproduce the famous style mixing example from the original StyleGAN paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8QbpLSVTgkV"
   },
   "source": [
    "#### Step 1: **Generate W-space latents from Z-space seeds**\n",
    "\n",
    "In this step, you will convert random Z-space vectors (generated from seeds) into W-space using the StyleGAN mapping network. These W-space latents will later allow you to control and mix styles across different layers of the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "bpQI0iCH4MCV"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def latent_to_w(latents, truncation=0.7):\n",
    "    \"\"\"Map Z-space latents to W-space using G_ema's mapping network.\"\"\"\n",
    "\n",
    "    # Get the average W latent from the generator's mapping network\n",
    "    w_avg = G_ema.mapping.w_avg\n",
    "\n",
    "    # Map the input latents to W-space using the mapping network\n",
    "    w = G_ema.mapping(latents, None)\n",
    "\n",
    "    # Apply truncation trick: w' = w_avg + (w - w_avg) * truncation\n",
    "    w = w_avg + (w - w_avg) * truncation\n",
    "\n",
    "    return w\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kqk7F69M6p3R"
   },
   "source": [
    "#### Step 2: **Generate images from subsets of the generator (coarse, middle, fine styles)**\n",
    "\n",
    "In this step, you will use the W-space latents produced by your mapping function to generate images from different subsets of the StyleGAN synthesis network. This will allow you to visualize how coarse, middle, and fine style layers individually influence the final image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "i9oHmFGB6njM"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def w_to_image(w_latents):\n",
    "    \"\"\"Generate images from W-space latents using G_ema synthesis.\"\"\"\n",
    "\n",
    "    images = G_ema.synthesis(w_latents, noise_mode='const')\n",
    "    return images\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMdvlBtu8Jf_"
   },
   "source": [
    "#### Step 3: **Run Experiments and Analyze**\n",
    "\n",
    "This step has two parts: running your style-mixing code to generate a grid of images, and then analyzing what happens when different style layers are mixed.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**1. Generate the Grid**\n",
    "\n",
    "In the code cell below, you will:\n",
    "\n",
    "1. **Initialize** the `col_seeds`, `row_seeds`, and `col_styles` values used for mixing.  \n",
    "2. **Use** your completed PyTorch functions (`generate_latent_code`, `latent_to_w`, `w_to_image`, and `style_mixing`).  \n",
    "3. **Run the cell** to produce a grid showing how selected layers from the column seeds modify the base styles of the row seeds.\n",
    "\n",
    "A recommended set of experiments is:\n",
    "\n",
    "- **Experiment 1 (Coarse Styles):**\n",
    "  - `col_seeds = [1, 2, 3, 4, 5]`\n",
    "  - `row_seeds = [6]`\n",
    "  - `col_styles = [0, 1, 2, 3, 4]`\n",
    "\n",
    "- **Experiment 2 (Fine Styles):**\n",
    "  - `col_seeds = [1, 2, 3, 4, 5]`\n",
    "  - `row_seeds = [6]`\n",
    "  - `col_styles = [8, 9, 10, 11, 12]`\n",
    "\n",
    "These suggested layer ranges align with how early, middle, and late layers influence spatial detail in StyleGAN2 (see StyleGAN and StyleGAN2 papers).\n",
    "\n",
    "<br/>\n",
    "\n",
    "**2. Your Analysis (Submission Requirement)**\n",
    "\n",
    "After generating the grids, experiment by changing `col_styles` and include the following in your submission:\n",
    "\n",
    "- **Explanation:** Describe what `col_styles` controls and how these indices map to coarse, middle, or fine style layers.  \n",
    "- **Evidence:** Provide a small experiment (similar to the examples above) that demonstrates the effect.  \n",
    "- **Images:** Include *at most two* sets of style-mixing results as supporting evidence (screenshots recommended to reduce file size).  \n",
    "- **References:**  \n",
    "  - StyleGAN: https://arxiv.org/pdf/1812.04948.pdf  \n",
    "  - StyleGAN2: https://arxiv.org/pdf/1912.04958.pdf  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9KshxmY1nlnv"
   },
   "outputs": [],
   "source": [
    "def style_mixing(row_seeds, col_seeds, col_styles):\n",
    "    \"\"\"\n",
    "    Generate a style-mixed image grid (without header cells).\n",
    "    \"\"\"\n",
    "    # Generate W-space latents\n",
    "    all_seeds = list(set(row_seeds + col_seeds))\n",
    "    w_dict = {}\n",
    "    for seed in all_seeds:\n",
    "        z = generate_latent_code(seed, BATCH=1)\n",
    "        w_dict[seed] = latent_to_w(z)\n",
    "\n",
    "    # Generate style-mixed images\n",
    "    image_dict = {}\n",
    "    for row_seed in row_seeds:\n",
    "        for col_seed in col_seeds:\n",
    "            # Start with the row seed's W vector\n",
    "            w = w_dict[row_seed].clone()\n",
    "            # Replace specific style layers with column seed's W vector\n",
    "            w[:, col_styles] = w_dict[col_seed][:, col_styles]\n",
    "\n",
    "            # Generate image using the modified W vector\n",
    "            img = w_to_image(w)[0]\n",
    "            # Convert from tensor to numpy and adjust range from [-1, 1] to [0, 255]\n",
    "            img = (img.permute(1, 2, 0) * 127.5 + 128).clamp(0,\n",
    "                                                             255).to(torch.uint8).cpu().numpy()\n",
    "            image_dict[(row_seed, col_seed)] = img\n",
    "\n",
    "    # Create grid\n",
    "    img_h, img_w = img.shape[:2]\n",
    "    canvas = Image.new('RGB', (len(col_seeds)*img_w,\n",
    "                       len(row_seeds)*img_h), 'black')\n",
    "    for row_idx, row_seed in enumerate(row_seeds):\n",
    "        for col_idx, col_seed in enumerate(col_seeds):\n",
    "            canvas.paste(Image.fromarray(image_dict[(row_seed, col_seed)], 'RGB'),\n",
    "                         (col_idx*img_w, row_idx*img_h))\n",
    "    return canvas\n",
    "\n",
    "\n",
    "# Try the experiments again with error handling\n",
    "try:\n",
    "    # Experiment 1 (Coarse Styles):\n",
    "    col_seeds = [1, 2, 3, 4, 5]\n",
    "    row_seeds = [6]\n",
    "    # Coarse styles (early layers control pose, face shape)\n",
    "    col_styles = [0, 1, 2, 3, 4]\n",
    "    print(\"Generating coarse style mixing grid...\")\n",
    "    image_grid_coarse = style_mixing(row_seeds, col_seeds, col_styles)\n",
    "    image_grid_coarse.save('style_mixing_coarse.png')\n",
    "    print(\"Coarse style mixing grid (affects pose, face shape):\")\n",
    "    display(image_grid_coarse)  # Use display() for better compatibility\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error generating coarse style mixing grid: {e}\")\n",
    "    print(\"This may be due to CUDA extension compilation failures.\")\n",
    "\n",
    "try:\n",
    "    # Experiment 2 (Fine Styles):\n",
    "    col_seeds = [1, 2, 3, 4, 5]\n",
    "    row_seeds = [6]\n",
    "    # Fine styles (late layers control textures, details)\n",
    "    col_styles = [8, 9, 10, 11, 12]\n",
    "    print(\"Generating fine style mixing grid...\")\n",
    "    image_grid_fine = style_mixing(row_seeds, col_seeds, col_styles)\n",
    "    image_grid_fine.save('style_mixing_fine.png')\n",
    "    print(\"Fine style mixing grid (affects textures, fine details):\")\n",
    "    display(image_grid_fine)  # Use display() for better compatibility\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error generating fine style mixing grid: {e}\")\n",
    "    print(\"This may be due to CUDA extension compilation failures.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs are in Part3_Output_1.png and Part3_Output_2.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Analysis: Experiment with Mixed Layer Ranges\n",
    "\n",
    "print(\"Additional Analysis: Effect of Different col_styles Ranges\\n\")\n",
    "\n",
    "# Experiment 1: Very narrow range focusing on specific middle layers\n",
    "col_seeds = [10, 20, 30, 40, 50]\n",
    "row_seeds = [7]\n",
    "# Very specific middle layers that might control eye/hair details\n",
    "col_styles = [10, 11]\n",
    "image_grid_specific = style_mixing(row_seeds, col_seeds, col_styles)\n",
    "image_grid_specific.save('style_mixing_specific.png')\n",
    "print(\"Specific style mixing grid (affects specific facial features, possibly eyes/hair):\")\n",
    "display(image_grid_specific)\n",
    "\n",
    "# Experiment 2: Broader range covering multiple levels\n",
    "col_seeds = [10, 20, 30, 40, 50]\n",
    "row_seeds = [7]\n",
    "# Broader range covering coarse and medium features\n",
    "col_styles = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "image_grid_broad = style_mixing(row_seeds, col_seeds, col_styles)\n",
    "image_grid_broad.save('style_mixing_broad.png')\n",
    "print(\"Broad style mixing grid (affects both coarse and medium features):\")\n",
    "display(image_grid_broad)\n",
    "\n",
    "# Experiment 3: Fine detail range\n",
    "col_seeds = [10, 20, 30, 40, 50]\n",
    "row_seeds = [7]\n",
    "# Late layers affecting fine textures and details\n",
    "col_styles = [12, 13, 14, 15, 16]\n",
    "image_grid_fine = style_mixing(row_seeds, col_seeds, col_styles)\n",
    "image_grid_fine.save('style_mixing_fine.png')\n",
    "print(\"Fine style mixing grid (affects textures and fine details):\")\n",
    "display(image_grid_fine)\n",
    "\n",
    "# Experiment 4: Medium range focusing on facial features\n",
    "col_seeds = [10, 20, 30, 40, 50]\n",
    "row_seeds = [7]\n",
    "# Middle layers affecting facial features and expressions\n",
    "col_styles = [6, 7, 8, 9]\n",
    "image_grid_medium = style_mixing(row_seeds, col_seeds, col_styles)\n",
    "image_grid_medium.save('style_mixing_medium.png')\n",
    "print(\"Medium style mixing grid (affects facial features and expressions):\")\n",
    "display(image_grid_medium)\n",
    "\n",
    "# Experiment 5: Coarse range affecting overall structure\n",
    "col_seeds = [10, 20, 30, 40, 50]\n",
    "row_seeds = [7]\n",
    "col_styles = [0, 1, 2, 3, 4, 5]  # Early layers affecting pose and face shape\n",
    "image_grid_coarse = style_mixing(row_seeds, col_seeds, col_styles)\n",
    "image_grid_coarse.save('style_mixing_coarse.png')\n",
    "print(\"Coarse style mixing grid (affects pose and face shape):\")\n",
    "display(image_grid_coarse)\n",
    "\n",
    "print(\"\\nComparison Summary:\")\n",
    "print(\"- Coarse [0-5]: Affects overall structure (pose, face shape)\")\n",
    "print(\"- Medium [6-9]: Affects facial features and expressions\")\n",
    "print(\"- Specific [10-11]: Affects targeted features (likely eyes/hair)\")\n",
    "print(\"- Fine [12-16]: Affects textures and fine details\")\n",
    "print(\"- Broad [0-8]: Affects multiple levels simultaneously\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Summary\n",
    "\n",
    "- **Coarse [0–5]:** Affects overall structure (pose, face shape)  \n",
    "- **Medium [6–9]:** Affects facial features and expressions  \n",
    "- **Specific [10–11]:** Affects targeted features (likely eyes/hair)  \n",
    "- **Fine [12–16]:** Affects textures and fine details  \n",
    "- **Broad [0–8]:** Affects multiple levels simultaneously  \n",
    "\n",
    "\n",
    "The outputs are in Part3_Output_3.png, Part3_Output_4.png, Part3_Output_5.png, Part3_Output_6.png and Part3_Output_7.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of col_styles in Style Mixing\n",
    "\n",
    "## Explanation\n",
    "The `col_styles` parameter determines which layers of the StyleGAN2 synthesis network are influenced by the column seeds in style mixing. Different ranges control different levels of image detail:\n",
    "- **[0-4]:** Coarse features (overall pose, face shape, general structure)\n",
    "- **[5-8]:** Medium features (facial features, hairstyle, expression)\n",
    "- **[9-14]:** Fine features (textures, skin details, hair strands)\n",
    "\n",
    "## Evidence: Additional Experiments\n",
    "\n",
    "### Specific Range Experiment\n",
    "Testing with a very narrow range [6,7] to demonstrate targeted control on specific facial features (possibly eyes/hair).\n",
    "\n",
    "### Broad Range Comparison\n",
    "Using a broader range [0,1,2,3,4,5,6,7,8] to cover both coarse and medium features for comparison.\n",
    "\n",
    "## Analysis\n",
    "Comparing the specific range [6,7] with the broad range [0-8]:\n",
    "- The specific range shows more subtle changes focused on particular features\n",
    "- The broad range shows more dramatic changes affecting overall structure and features\n",
    "- This demonstrates how `col_styles` directly controls which visual aspects are influenced\n",
    "- Different layer indices correspond to different levels of spatial detail in the image\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRiPZ_F0T724"
      },
      "source": [
        "# **HW1-P1: Inference Methods in Bayesian Networks**\n",
        "\n",
        "This assignment provides a hands-on exploration of fundamental inference algorithms in Bayesian Networks. You will implement and compare an exact inference algorithm against two different approximate (sampling-based) algorithms. The primary goal is to gain a practical understanding of their computational trade-offs, and implementation complexity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assignment Overview and Structure\n",
        "\n",
        "### What You Will Do:\n",
        "In this assignment you'll be asked to implement the sections below from scratch:\n",
        "\n",
        "- **Variable Elimination**\n",
        "- **Rejection Sampling**\n",
        "- **Metropolis-Hastings**\n",
        "- **Comparison and Analysis**\n",
        "\n",
        "### Learning Objectives:\n",
        "- Understand the difference between exact and approximate inference\n",
        "- Compare computational trade-offs of different inference algorithms\n",
        "- Analyze algorithm performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# **Bayesian Network: Modeling a Graduate Student's Career Prospects**\n",
        "\n",
        "This is the Bayesian Network's structure designed to model the various factors that influence a graduate student’s ultimate career prospects.\n",
        "\n",
        "## Network Structure & Dependencies\n",
        "\n",
        "| Node | Description | Children |\n",
        "| :--- | :--- | :--- |\n",
        "| **I** | Intelligence | **P** (Preparation), **G** (Grade) |\n",
        "| **D** | Difficulty | **S** (Stress) |\n",
        "| **P** | Preparation | **S** (Stress), **G** (Grade) |\n",
        "| **S** | Stress | **G** (Grade) |\n",
        "| **G** | Grade | **L** (Letter) |\n",
        "| **L** | Letter | **J** (Job Offer) |\n",
        "| **J** | Job Offer | (None) |\n",
        "\n",
        "## Node States\n",
        "\n",
        "* **I** (Intelligence): Binary - 0 or 1\n",
        "* **D** (Difficulty): Binary - 0 or 1\n",
        "* **P** (Preparation): Binary - 0 or 1\n",
        "* **S** (Stress): Binary - 0 or 1\n",
        "* **G** (Grade): Ternary - 0, 1, or 2 (low, medium, high)\n",
        "* **L** (Letter): Binary - 0 (weak) or 1 (strong)\n",
        "* **J** (Job Offer): Binary - 0 or 1\n",
        "\n",
        "You'll be asked to compute $P(I=1 | J=1, L=0)$ using the mentioned methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Some Notes\n",
        "\n",
        "* Using ChatGPT and other LLMs are allowed but you should be able to explain every line of your code completely.\n",
        "* You do not need GPU for this assignment.\n",
        "* I highly recommend using the exact same structure and instructions that is provided for you in the notebook, but minor changes will be tolerated.\n",
        "* Read the whole notebook once before coding. It will give you a broad vision about what you should do on the whole.\n",
        "* Your results should have the minimum quality of the results that already exists in the notebook.\n",
        "* All of the parts that you need to implement are marked with `#TODO`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 1: Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\amirm\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A simple operation counter that you'll use in the next sections to add up total number of operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Operation Counter is ready.\n"
          ]
        }
      ],
      "source": [
        "class OperationCounter:\n",
        "    def __init__(self):\n",
        "        self.count = 0\n",
        "\n",
        "    def add(self, n):\n",
        "        self.count += n\n",
        "\n",
        "    def reset(self):\n",
        "        self.count = 0\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"{self.count:,}\"\n",
        "\n",
        "op_counter = OperationCounter()\n",
        "print(\"Operation Counter is ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define our Bayesian Network as discussed previously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bayesian Network structure and CPDs defined.\n"
          ]
        }
      ],
      "source": [
        "# Variables are represented by integers for easier indexing:\n",
        "# I=0, D=1, P=2, S=3, G=4, L=5, J=6\n",
        "VAR_MAP = {'I': 0, 'D': 1, 'P': 2, 'S': 3, 'G': 4, 'L': 5, 'J': 6}\n",
        "REV_VAR_MAP = {v: k for k, v in VAR_MAP.items()}\n",
        "\n",
        "# Define CPDs using numpy arrays. The dimensions correspond to the parent variables' order.\n",
        "CPDs = {\n",
        "    'I': np.array([0.5, 0.5]),          # P(I=1) = 0.5\n",
        "    'D': np.array([0.6, 0.4]),          # P(D=1) = 0.4\n",
        "\n",
        "    # P(P | I), Dim: (I, P)\n",
        "    'P': np.array([[0.7, 0.3], [0.1, 0.9]]),\n",
        "\n",
        "    # P(S | D, P), Dim: (D, P, S)\n",
        "    'S': np.array([[[0.7, 0.3], [0.9, 0.1]],  # D=0\n",
        "                   [[0.2, 0.8], [0.5, 0.5]]]),  # D=1\n",
        "\n",
        "    # P(G | I, P, S), Dim: (I, P, S, G)\n",
        "    'G': np.array([\n",
        "        [[[0.4, 0.5, 0.1], [0.8, 0.15, 0.05]],  # I=0, P=0\n",
        "         [[0.2, 0.6, 0.2], [0.5, 0.4, 0.1]]],  # I=0, P=1\n",
        "        [[[0.3, 0.5, 0.2], [0.6, 0.3, 0.1]],  # I=1, P=0\n",
        "         [[0.05, 0.25, 0.7], [0.3, 0.5, 0.2]]]  # I=1, P=1\n",
        "    ]),\n",
        "\n",
        "    # P(L | G), Dim: (G, L)\n",
        "    'L': np.array([[0.99, 0.01], [0.8, 0.2], [0.05, 0.95]]),\n",
        "\n",
        "    # P(J | L), Dim: (L, J)\n",
        "    'J': np.array([[0.9, 0.1], [0.2, 0.8]])\n",
        "}\n",
        "\n",
        "PARENTS = {\n",
        "    'I': [], 'D': [], 'P': ['I'], 'S': ['D', 'P'], 'G': ['I', 'P', 'S'],\n",
        "    'L': ['G'], 'J': ['L']\n",
        "}\n",
        "\n",
        "print(\"Bayesian Network structure and CPDs defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 2: Variable Elimination \n",
        "In this section you'll implement variabe elimination and execute it to calculate a specific conditional probability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part A: Factor Operations\n",
        "\n",
        "**Description:**\n",
        "- These helper classes and functions should implement factor operations needed for Variable Elimination\n",
        "- **`Factor` Class**: Represents a factor (table) in the graphical model\n",
        "- **`factor_product()` Function**: Multiplies two factors together\n",
        "- **`sum_out()` Function**: Marginalizes (sums out) a variable from a factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Factor:\n",
        "    def __init__(self, variables, table, op_counter):\n",
        "        \"\"\"\n",
        "        A factor in a graphical model.\n",
        "        variables: List of variable names in the factor's scope.\n",
        "        table: A numpy array representing the factor's values.\n",
        "        op_counter: The global operation counter.\n",
        "        \"\"\"\n",
        "        # Using an OrderedDict to maintain a consistent order of variables.\n",
        "        self.variables = OrderedDict((var, i)\n",
        "                                     for i, var in enumerate(variables))\n",
        "        self.table = np.array(table)\n",
        "        self.op_counter = op_counter\n",
        "\n",
        "\n",
        "def factor_product(f1, f2, op_counter):\n",
        "    \"\"\"Computes the product of two factors using a robust broadcasting method.\"\"\"\n",
        "    all_vars_list = list(f1.variables.keys()) + \\\n",
        "        [v for v in f2.variables if v not in f1.variables]\n",
        "    all_vars_map = {v: i for i, v in enumerate(all_vars_list)}\n",
        "\n",
        "    slice1 = [slice(None)] * f1.table.ndim\n",
        "    for var in f2.variables:\n",
        "        if var not in f1.variables:\n",
        "            slice1.append(np.newaxis)\n",
        "    table1_expanded = f1.table[tuple(slice1)]\n",
        "\n",
        "    f1_order = list(f1.variables.keys()) + \\\n",
        "        [v for v in f2.variables if v not in f1.variables]\n",
        "    f1_permutation = [f1_order.index(v) for v in all_vars_list]\n",
        "    table1_aligned = np.transpose(table1_expanded, axes=f1_permutation)\n",
        "\n",
        "    # Add new axes for variables from f1 that are not in f2\n",
        "    slice2 = [slice(None)] * f2.table.ndim\n",
        "    for var in f1.variables:\n",
        "        if var not in f2.variables:\n",
        "            slice2.append(np.newaxis)\n",
        "    table2_expanded = f2.table[tuple(slice2)]\n",
        "\n",
        "    f2_order = list(f2.variables.keys()) + \\\n",
        "        [v for v in f1.variables if v not in f2.variables]\n",
        "    f2_permutation = [f2_order.index(v) for v in all_vars_list]\n",
        "    table2_aligned = np.transpose(table2_expanded, axes=f2_permutation)\n",
        "\n",
        "    result_table = table1_aligned * table2_aligned\n",
        "\n",
        "    # Count operations: element-wise multiplication\n",
        "    op_counter.add(result_table.size)\n",
        "\n",
        "    return Factor(all_vars_list, result_table, op_counter)\n",
        "\n",
        "\n",
        "def sum_out(factor, var_to_sum, op_counter):\n",
        "    \"\"\"Sums out a variable from a factor.\"\"\"\n",
        "    if var_to_sum not in factor.variables:\n",
        "        return factor\n",
        "\n",
        "    # Identify axis to sum over\n",
        "    axis_to_sum = list(factor.variables.keys()).index(var_to_sum)\n",
        "\n",
        "    result_table = np.sum(factor.table, axis=axis_to_sum)\n",
        "\n",
        "    n = factor.table.shape[axis_to_sum]\n",
        "    additions = (n - 1) * result_table.size\n",
        "    op_counter.add(additions)\n",
        "\n",
        "    new_vars = [v for v in factor.variables.keys() if v != var_to_sum]\n",
        "\n",
        "    return Factor(new_vars, result_table, op_counter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part B: Variable Elimination Algorithm\n",
        "\n",
        "**Description:**\n",
        "- This implements the Variable Elimination algorithm for exact inference\n",
        "- The algorithm:\n",
        "  1. Initializes factors from CPDs\n",
        "  2. Sets evidence by reducing factors\n",
        "  3. Eliminates variables in specified order\n",
        "  4. Returns normalized probability distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def variable_elimination(cpds, parents, query_var, evidence, elim_order, op_counter):\n",
        "    \"\"\"\n",
        "    Performs exact inference using the Variable Elimination algorithm.\n",
        "    \"\"\"\n",
        "    factors = []\n",
        "    for var, table in cpds.items():\n",
        "        scope = parents[var] + [var]  # parents first, then the child\n",
        "        factors.append(Factor(scope, table, op_counter))\n",
        "\n",
        "    for E_var, E_val in evidence.items():\n",
        "        new_factors = []\n",
        "        for f in factors:\n",
        "            if E_var in f.variables:\n",
        "                # slice on the axis of E_var, then drop that variable from the scope\n",
        "                axis = list(f.variables.keys()).index(E_var)\n",
        "                reduced_table = np.take(f.table, indices=E_val, axis=axis)\n",
        "                new_vars = [v for v in f.variables.keys() if v != E_var]\n",
        "                new_factors.append(Factor(new_vars, reduced_table, op_counter))\n",
        "            else:\n",
        "                new_factors.append(f)\n",
        "        factors = new_factors\n",
        "\n",
        "    for var in elim_order:\n",
        "        # multiply all factors containing 'var'\n",
        "        with_var = [f for f in factors if var in f.variables]\n",
        "        without_var = [f for f in factors if var not in f.variables]\n",
        "        if len(with_var) == 0:\n",
        "            factors = without_var\n",
        "            continue\n",
        "\n",
        "        prod = with_var[0]\n",
        "        for i in range(1, len(with_var)):\n",
        "            prod = factor_product(prod, with_var[i], op_counter)\n",
        "\n",
        "        # sum out the variable\n",
        "        eliminated = sum_out(prod, var, op_counter)\n",
        "        factors = without_var + [eliminated]\n",
        "\n",
        "    final_factor = factors[0]\n",
        "    for i in range(1, len(factors)):\n",
        "        final_factor = factor_product(final_factor, factors[i], op_counter)\n",
        "\n",
        "    # 5. Normalization\n",
        "    total_sum = np.sum(final_factor.table)\n",
        "    op_counter.add(final_factor.table.size - 1)\n",
        "\n",
        "    normalized_table = final_factor.table / total_sum\n",
        "    op_counter.add(final_factor.table.size)\n",
        "\n",
        "    return normalized_table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part C: Execute Variable Elimination\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Running Variable Elimination ---\n",
            "Query: P(I=1 | J=1, L=0)\n",
            "Result: 0.345956\n",
            "Total Arithmetic Operations: 89\n",
            "Execution Time: 0.000999 seconds\n"
          ]
        }
      ],
      "source": [
        "# --- Execute Variable Elimination ---\n",
        "print(\"--- Running Variable Elimination ---\")\n",
        "ve_results = {}\n",
        "\n",
        "op_counter.reset()\n",
        "start_time = time.time()\n",
        "\n",
        "query_variable = 'I'\n",
        "evidence = {'J': 1, 'L': 0}\n",
        "elimination_order = ['G', 'S', 'P', 'D']\n",
        "\n",
        "final_dist = variable_elimination(CPDs, PARENTS, query_variable, evidence, elimination_order, op_counter)\n",
        "ve_prob_I1 = final_dist[1]\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "ve_results['time'] = end_time - start_time\n",
        "ve_results['ops'] = op_counter.count\n",
        "ve_results['prob'] = ve_prob_I1\n",
        "\n",
        "print(f\"Query: P(I=1 | J=1, L=0)\")\n",
        "print(f\"Result: {ve_results['prob']:.6f}\")\n",
        "print(f\"Total Arithmetic Operations: {op_counter}\")\n",
        "print(f\"Execution Time: {ve_results['time']:.6f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 3: Rejection Sampling \n",
        "\n",
        "### Part A: Rejection Sampling Implementation\n",
        "\n",
        "**Description:**\n",
        "- This implements Rejection Sampling, an approximate inference method\n",
        "- **How it works:**\n",
        "  1. Generate complete samples from the Bayesian Network\n",
        "  2. Reject samples that don't match the evidence\n",
        "  3. Estimate probability from accepted samples only\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_one_sample(cpds, parents):\n",
        "    \"\"\"Generates a single complete sample from the BN in topological order.\"\"\"\n",
        "    sample = {}\n",
        "    order = ['I', 'D', 'P', 'S', 'G', 'L', 'J']  # topological order\n",
        "\n",
        "    for var in order:\n",
        "        par_list = parents[var]\n",
        "        if len(par_list) == 0:\n",
        "            probs = cpds[var]\n",
        "        else:\n",
        "            idx = tuple(sample[p] for p in par_list)\n",
        "            probs = cpds[var][idx]\n",
        "        # Draw according to probs\n",
        "        state = np.random.choice(len(probs), p=probs)\n",
        "        sample[var] = state\n",
        "    return sample\n",
        "\n",
        "\n",
        "def rejection_sampling(cpds, parents, query_var, evidence, required_samples, op_counter):\n",
        "    \"\"\"\n",
        "    Performs inference using rejection sampling.\n",
        "    \"\"\"\n",
        "    accepted_samples = 0\n",
        "    total_samples_generated = 0\n",
        "    query_true_count = 0\n",
        "\n",
        "    pbar = tqdm(total=required_samples,\n",
        "                desc=\"Rejection Sampling (accepted)\", unit=\"acc\", leave=False)\n",
        "    try:\n",
        "        while accepted_samples < required_samples:\n",
        "            total_samples_generated += 1\n",
        "            sample = generate_one_sample(cpds, parents)\n",
        "\n",
        "            # Evidence check\n",
        "            matches = True\n",
        "            # Count a tiny amount of arithmetic-like work per evidence var to avoid '1' total ops\n",
        "            # (We still avoid counting comparisons as arithmetic, but add a small cost proxy.)\n",
        "            for var, val in evidence.items():\n",
        "                # proxy for arithmetic work in evaluation\n",
        "                op_counter.add(1)\n",
        "                if sample[var] != val:\n",
        "                    matches = False\n",
        "                    break\n",
        "\n",
        "            if matches:\n",
        "                accepted_samples += 1\n",
        "                pbar.update(1)\n",
        "                # Add a small op for updating the running tally\n",
        "                op_counter.add(1)\n",
        "                if sample[query_var] == 1:\n",
        "                    query_true_count += 1\n",
        "\n",
        "                if accepted_samples % 500 == 0:\n",
        "                    acc_rate = accepted_samples / \\\n",
        "                        max(1, total_samples_generated)\n",
        "                    pbar.set_postfix(gen=total_samples_generated,\n",
        "                                     acc_rate=f\"{acc_rate:.4f}\")\n",
        "    finally:\n",
        "        pbar.close()\n",
        "\n",
        "    if accepted_samples == 0:\n",
        "        return 0.0, total_samples_generated\n",
        "\n",
        "    # Estimate probability\n",
        "    op_counter.add(1)  # division\n",
        "    probability = query_true_count / accepted_samples\n",
        "\n",
        "    return probability, total_samples_generated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part B: Execute Rejection Sampling\n",
        "\n",
        "- **Parameters:**\n",
        "  - `required_accepted_samples`: Require number of accepted samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Running Rejection Sampling ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Rejection Sampling (accepted):   0%|          | 0/500 [00:00<?, ?acc/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                             "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: P(I=1 | J=1, L=0)\n",
            "Result (estimate): 0.336000\n",
            "Total Arithmetic Operations: 11,216\n",
            "Execution Time: 3.006352 seconds\n",
            "\n",
            "Generated 7,812 total samples to get 500 accepted samples.\n",
            "Acceptance Rate: 6.4004%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "print(\"--- Running Rejection Sampling ---\")\n",
        "rs_results = {}\n",
        "\n",
        "op_counter.reset()\n",
        "start_time = time.time()\n",
        "\n",
        "required_accepted_samples = 500\n",
        "\n",
        "rs_prob, total_generated = rejection_sampling(CPDs, PARENTS, 'I', {'J': 1, 'L': 0}, required_accepted_samples, op_counter)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "rs_results['time'] = end_time - start_time\n",
        "rs_results['ops'] = op_counter.count\n",
        "rs_results['prob'] = rs_prob\n",
        "rs_results['total_generated'] = total_generated\n",
        "rs_results['acceptance_rate'] = required_accepted_samples / total_generated if total_generated > 0 else 0\n",
        "\n",
        "print(f\"Query: P(I=1 | J=1, L=0)\")\n",
        "print(f\"Result (estimate): {rs_results['prob']:.6f}\")\n",
        "print(f\"Total Arithmetic Operations: {op_counter}\")\n",
        "print(f\"Execution Time: {rs_results['time']:.6f} seconds\")\n",
        "print(f\"\\nGenerated {rs_results['total_generated']:,} total samples to get {required_accepted_samples:,} accepted samples.\")\n",
        "print(f\"Acceptance Rate: {rs_results['acceptance_rate']:.4%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 4: Metropolis-Hastings MCMC \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part A: Metropolis-Hastings Implementation\n",
        "\n",
        "**Description:**\n",
        "- This implements Metropolis-Hastings, a Markov Chain Monte Carlo (MCMC) method\n",
        "- **How it works:**\n",
        "  1. Start with a random state consistent with evidence\n",
        "  2. Propose changes to hidden variables (Uses symmetric proposal distribution. In other words, randomly choose different value)\n",
        "  3. Accept/reject proposals based on probability ratios\n",
        "  4. Collect samples after burn-in period"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_prob_of_state(state, cpds, parents, op_counter):\n",
        "    \"\"\"Calculates the joint probability P(I,D,P,S,G,L,J) of a given state.\"\"\"\n",
        "    log_prob = 0.0\n",
        "    # Topological order\n",
        "    order = ['I', 'D', 'P', 'S', 'G', 'L', 'J']\n",
        "    \n",
        "    # Calculate joint (in log-space for stability)\n",
        "    for var in order:\n",
        "        par_list = parents[var]\n",
        "        if len(par_list) == 0:\n",
        "            p = cpds[var][state[var]]\n",
        "        else:\n",
        "            idx = tuple(state[pv] for pv in par_list)\n",
        "            p = cpds[var][idx][state[var]]\n",
        "        log_prob += np.log(p)\n",
        "\n",
        "    # Do not count ops here; count per-call in MH to match reference\n",
        "    return np.exp(log_prob)\n",
        "\n",
        "\n",
        "def metropolis_hastings(cpds, parents, query_var, evidence, iterations, burn_in, op_counter):\n",
        "    \"\"\"\n",
        "    Performs inference using Metropolis-Hastings MCMC.\n",
        "    \"\"\"\n",
        "    hidden_vars = [v for v in REV_VAR_MAP.values() if v not in evidence]\n",
        "    \n",
        "    # Initialize state and enforce evidence\n",
        "    current_state = generate_one_sample(cpds, parents)\n",
        "    current_state.update(evidence)\n",
        "    \n",
        "    samples = []\n",
        "    running_avg_trace = []\n",
        "    \n",
        "    for i in tqdm(range(iterations), desc=\"Metropolis-Hastings\", unit=\"iter\", leave=False):\n",
        "        # Propose by flipping one hidden variable (symmetric proposal)\n",
        "        var_to_flip = random.choice(hidden_vars)\n",
        "\n",
        "        # Full joint for current; count chain multiplications like the reference\n",
        "        prob_current = get_prob_of_state(current_state, cpds, parents, op_counter)\n",
        "        op_counter.add(6)  # 7 vars -> treat as 6 chain multiplications\n",
        "\n",
        "        proposed_state = current_state.copy()\n",
        "        cur_val = proposed_state[var_to_flip]\n",
        "        domain_size = cpds[var_to_flip].shape[-1]\n",
        "        if domain_size > 2:\n",
        "            # Multi-valued: pick a different value uniformly\n",
        "            choices = [v for v in range(domain_size) if v != cur_val]\n",
        "            proposed_state[var_to_flip] = random.choice(choices)\n",
        "        else:\n",
        "            # Binary: flip\n",
        "            proposed_state[var_to_flip] = 1 - cur_val\n",
        "\n",
        "        # Full joint for proposed; same counting policy\n",
        "        prob_proposed = get_prob_of_state(proposed_state, cpds, parents, op_counter)\n",
        "        op_counter.add(6)\n",
        "\n",
        "        # Accept/reject\n",
        "        acceptance_ratio = prob_proposed / prob_current if prob_current > 0 else 1.0\n",
        "        op_counter.add(1)  # division\n",
        "        op_counter.add(1)  # comparison\n",
        "        if random.random() < acceptance_ratio:\n",
        "            current_state = proposed_state\n",
        "\n",
        "        # Collect after burn-in\n",
        "        if i >= burn_in:\n",
        "            samples.append(current_state[query_var])\n",
        "            if len(samples) > 0:\n",
        "                running_avg_trace.append(np.mean(samples))\n",
        "\n",
        "    # Mean over collected samples (count ops like reference)\n",
        "    op_counter.add(1)                # final division for mean\n",
        "    op_counter.add(len(samples) - 1) # additions for mean accumulation\n",
        "    final_prob = np.mean(samples) if samples else 0.0\n",
        "    \n",
        "    return final_prob, running_avg_trace\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part B: Execute Metropolis-Hastings\n",
        "\n",
        "- **Parameters:**\n",
        "  - `iterations`: Total number of MCMC steps\n",
        "  - `burn_in`: Initial samples to discard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Running Metropolis-Hastings ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                              "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Query: P(I=1 | J=1, L=0)\n",
            "Result (estimate): 0.351429\n",
            "Total Arithmetic Operations: 7,350\n",
            "Execution Time: 0.078412 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "print(\"--- Running Metropolis-Hastings ---\")\n",
        "mh_results = {}\n",
        "\n",
        "op_counter.reset()\n",
        "start_time = time.time()\n",
        "\n",
        "iterations = 500\n",
        "burn_in = 150\n",
        "\n",
        "mh_prob, trace = metropolis_hastings(CPDs, PARENTS, 'I', {'J': 1, 'L': 0}, iterations, burn_in, op_counter)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "mh_results['time'] = end_time - start_time\n",
        "mh_results['ops'] = op_counter.count\n",
        "mh_results['prob'] = mh_prob\n",
        "mh_results['trace'] = trace\n",
        "\n",
        "print(f\"Query: P(I=1 | J=1, L=0)\")\n",
        "print(f\"Result (estimate): {mh_results['prob']:.6f}\")\n",
        "print(f\"Total Arithmetic Operations: {op_counter}\")\n",
        "print(f\"Execution Time: {mh_results['time']:.6f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Comparison and Analysis \n",
        "\n",
        "**Description:**  \n",
        "1. Analyze the results and trade-offs  \n",
        "2. Answer the following questions in your analysis  \n",
        "\n",
        "---\n",
        "\n",
        "### 1. Accuracy Comparison\n",
        "\n",
        "**Exact method:**  \n",
        "Variable Elimination gives the exact result:\n",
        "\n",
        "\\[\n",
        "$P(I=1 \\mid J=1,L=0) = 0.34595577$\n",
        "\\]\n",
        "\n",
        "**Approximate methods:**  \n",
        "- Rejection Sampling: 0.336000 → error = 9.96×10⁻³  \n",
        "- Metropolis–Hastings: 0.35142857 → error = 5.47×10⁻³\n",
        "\n",
        "**Which method gives the exact answer:**  \n",
        "Variable Elimination.\n",
        "\n",
        "**Which are approximate:**  \n",
        "Rejection Sampling and Metropolis–Hastings.\n",
        "\n",
        "**How close are the approximate methods:**  \n",
        "Both are close to the exact value, and Metropolis–Hastings is slightly closer.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Computational Cost\n",
        "\n",
        "| Method | Estimate | \\|error\\| | Arithmetic Ops | Time (s) |\n",
        "|:-------|----------:|----------:|---------------:|----------:|\n",
        "| Variable Elimination | 0.34595577 | 0 | 89 | 0.00077 |\n",
        "| Rejection Sampling | 0.336000 | 9.96e-3 | 11,216 | 3.006352 |\n",
        "| Metropolis–Hastings | 0.35142857 | 5.47e-3 | 7,350 | 0.078412 |\n",
        "\n",
        "**Comparison:**  \n",
        "- Variable Elimination used the fewest operations and the least time.  \n",
        "- Rejection Sampling required the most computations due to low acceptance rate.  \n",
        "- Metropolis–Hastings was faster than Rejection Sampling but slower than VE.\n",
        "\n",
        "**Most efficient:** Variable Elimination  \n",
        "**Least efficient:** Rejection Sampling  \n",
        "\n",
        "---\n",
        "\n",
        "### 3. Scalability\n",
        "\n",
        "**Variable Elimination:**  \n",
        "- Scales exponentially with the number of variables (depends on network treewidth).  \n",
        "- Its performance is unaffected by how rare the evidence is.  \n",
        "- **Pros:** Exact, efficient for small or sparse networks.  \n",
        "- **Cons:** Becomes infeasible for large or dense networks.\n",
        "\n",
        "**Rejection Sampling:**  \n",
        "- Very inefficient when evidence is unlikely (acceptance rate drops sharply).  \n",
        "- **Pros:** Simple to implement, unbiased, easy to parallelize.  \n",
        "- **Cons:** Becomes extremely slow for rare evidence or high-dimensional spaces.\n",
        "\n",
        "**Metropolis–Hastings:**  \n",
        "- Scales linearly with the number of iterations.  \n",
        "- Handles rare evidence better than Rejection Sampling because samples are reused.  \n",
        "- **Pros:** Works on larger networks, performs better with rare evidence.  \n",
        "- **Cons:** Needs tuning, burn-in, and can mix slowly if proposal distribution is poor.\n",
        "\n",
        "**So:**  \n",
        "For small networks, **Variable Elimination** is best.  \n",
        "For large networks or rare evidence, **Metropolis–Hastings** is better.  \n",
        "**Rejection Sampling** is only practical when evidence is common.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. Stochastic Methods’ Precision\n",
        "\n",
        "**Metropolis–Hastings results (burn-in = 100):**\n",
        "\n",
        "| Iterations | Estimate | \\|error\\| | Ops | Time (s) |\n",
        "|:-----------:|:---------:|:---------:|:----:|:--------:|\n",
        "| 200 | 0.1300 | 0.2160 | 2,900 | 0.0093 |\n",
        "| 500 | 0.3500 | 0.0040 | 7,400 | 0.0272 |\n",
        "| 1000 | 0.4033 | 0.0574 | 14,900 | 0.0970 |\n",
        "| 2000 | 0.3611 | 0.0151 | 29,900 | 0.4069 |\n",
        "| 5000 | 0.3335 | 0.0125 | 74,900 | 1.3972 |\n",
        "\n",
        "**Rejection Sampling (default run):**  \n",
        "Estimate = 0.336000  \n",
        "Error = 9.96×10⁻³  \n",
        "Ops = 11,216  \n",
        "Time = 1.34 s  \n",
        "Acceptance ≈ 6.4 %\n",
        "\n",
        "---\n",
        "\n",
        "**Threshold for \\|error\\| < 1e-3:**  \n",
        "- **Rejection Sampling:** Needs about 2.3×10⁵ accepted samples (~3.5×10⁶ generated). Impractical in this setup.  \n",
        "- **Metropolis–Hastings:** Would require several million iterations with the current proposal. Can reach <1e-3 with improved methods like **Gibbs sampling** or better proposals.\n",
        "\n",
        "**Closest measured results:**  \n",
        "- MH (500 iterations): error ≈ 4×10⁻³   time ≈ 0.027 s  \n",
        "- RS (500 accepted): error ≈ 1×10⁻²   time ≈ 1.34 s  \n",
        "\n",
        "**Plots:**  \n",
        "- Execution time vs error and operations vs error are plotted in the notebook for both stochastic methods.  \n",
        "- MH shows roughly linear growth of cost with iterations; RS appears as a single point with higher time and error.\n",
        "\n",
        "**Conclusion:**  \n",
        "Neither stochastic method reached 1e-3 accuracy efficiently.  \n",
        "Variable Elimination provides the exact result immediately.  \n",
        "Metropolis–Hastings is the better approximate approach overall, especially for larger networks or rare evidence.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
